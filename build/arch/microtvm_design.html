
<!DOCTYPE html>

<html lang="zh_CN">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>microTVM Design Document &#8212; tvm-cn 1.0.0 文档</title>
    <link rel="stylesheet" href="../_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/language_data.js"></script>
    <script src="../_static/translations.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="index" title="索引" href="../genindex.html" />
    <link rel="search" title="搜索" href="../search.html" />
    <link rel="next" title="microTVM Project API" href="microtvm_project_api.html" />
    <link rel="prev" title="Security Guide" href="security.html" />
   
  <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <div class="section" id="microtvm-design-document">
<span id="microtvm-design"></span><h1><a class="toc-backref" href="#id1">microTVM Design Document</a><a class="headerlink" href="#microtvm-design-document" title="永久链接至标题">¶</a></h1>
<div class="contents topic" id="table-of-contents">
<p class="topic-title">Table of Contents</p>
<ul class="simple">
<li><p><a class="reference internal" href="#microtvm-design-document" id="id1">microTVM Design Document</a></p>
<ul>
<li><p><a class="reference internal" href="#background" id="id2">Background</a></p></li>
<li><p><a class="reference internal" href="#typical-use" id="id3">Typical Use</a></p></li>
<li><p><a class="reference internal" href="#design-goals" id="id4">Design Goals</a></p></li>
<li><p><a class="reference internal" href="#overview" id="id5">Overview</a></p>
<ul>
<li><p><a class="reference internal" href="#modeling-target-platforms" id="id6">Modeling Target Platforms</a></p></li>
<li><p><a class="reference internal" href="#tvm-targets-for-microtvm" id="id7">TVM Targets for microTVM</a></p></li>
<li><p><a class="reference internal" href="#runtime-and-executor-configuration-for-microtvm" id="id8">Runtime and Executor configuration for microTVM</a></p></li>
<li><p><a class="reference internal" href="#writing-schedules-for-microtvm" id="id9">Writing Schedules for microTVM</a></p></li>
<li><p><a class="reference internal" href="#executing-models" id="id10">Executing Models</a></p></li>
<li><p><a class="reference internal" href="#microtvm-firmware" id="id11">microTVM Firmware</a></p></li>
<li><p><a class="reference internal" href="#parts-of-a-microtvm-binary" id="id12">Parts of a microTVM Binary</a></p></li>
<li><p><a class="reference internal" href="#the-automated-build-flow" id="id13">The Automated Build Flow</a></p></li>
<li><p><a class="reference internal" href="#measuring-operator-performance" id="id14">Measuring operator performance</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#future-work" id="id15">Future Work</a></p>
<ul>
<li><p><a class="reference internal" href="#ahead-of-time-runtime" id="id16">Ahead-of-Time Runtime</a></p></li>
<li><p><a class="reference internal" href="#memory-planning" id="id17">Memory Planning</a></p></li>
<li><p><a class="reference internal" href="#heterogeneous-execution" id="id18">Heterogeneous Execution</a></p></li>
<li><p><a class="reference internal" href="#autotuning-target" id="id19">Autotuning Target</a></p></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
<div class="section" id="background">
<h2><a class="toc-backref" href="#id2">Background</a><a class="headerlink" href="#background" title="永久链接至标题">¶</a></h2>
<p>TVM is a model deployment framework that has demonstrated good performance across a wide range of
models on traditional operating systems. Given TVM’s layered approach to compilation, it is a
natural extension to target bare metal devices. While most of the compilation flow does not need to
change for a proof-of-concept implementation on such devices, the runtime cannot depend on:</p>
<ul class="simple">
<li><p><strong>Virtual Memory</strong>, and by extension any system-provided <code class="docutils literal notranslate"><span class="pre">malloc</span></code>. Additionally, bare metal
devices typically have very limited memory (measured in KB). Because of this, libraries designed
for such platforms typically need to be more judicious in using memory, and need to release
memory when it is not in use.</p></li>
<li><p>Traditional OS abstractions, such as <strong>files</strong>, <strong>libraries</strong>, and <strong>kernel functions</strong>. Some
projects implement support for these, but they are by no means standard.</p></li>
<li><p>Support for programming languages other than <strong>C</strong>.</p></li>
</ul>
<p>Such changes require a different approach from the TVM C++ runtime typically used on traditional
Operating Systems.</p>
</div>
<div class="section" id="typical-use">
<h2><a class="toc-backref" href="#id3">Typical Use</a><a class="headerlink" href="#typical-use" title="永久链接至标题">¶</a></h2>
<p>This section discusses our vision of the “typical” microTVM use case. Each component used to achieve
this typical use case is intended to be designed for flexibility, but this unifying vision serves to
motivate the inclusion of each part of the design.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="https://raw.githubusercontent.com/tvmai/web-data/main/images/dev/microtvm_workflow.svg"><img alt="https://raw.githubusercontent.com/tvmai/web-data/main/images/dev/microtvm_workflow.svg" src="https://raw.githubusercontent.com/tvmai/web-data/main/images/dev/microtvm_workflow.svg" width="85%" /></a>
</div>
<p>The parts of this process are described below:</p>
<ol class="arabic simple">
<li><p><strong>Model Import</strong>. The user imports an existing model or describes a new model to TVM, producing a
<em>Relay module</em>.</p></li>
<li><p><strong>Model Transformations</strong>. The user can apply transformations, such as quantization, to the
model. After each transformation, the user should still have a Relay module.</p></li>
<li><p><strong>Compilation</strong> (Scheduling and Code Generation). TVM implements each operator into Tensor IR by
assigning a schedule and schedule configuration to each Relay operator. Then, code (C source or
compiled object) is generated for each operator.</p></li>
<li><p><strong>Integration</strong>. The generated code is integrated along with the TVM C Runtime library into a
user-supplied binary project. In some cases (such as when the project is standardized across
multiple SoC/development boards), this process is handled automatically.</p></li>
<li><p><strong>Deployment</strong>. The project is built and the residual firmware binary is flashed onto the device.
Model inference is driven either by TVM using an on-device RPC server, or on the device using the
on-device Graph Executor.</p></li>
</ol>
</div>
<div class="section" id="design-goals">
<h2><a class="toc-backref" href="#id4">Design Goals</a><a class="headerlink" href="#design-goals" title="永久链接至标题">¶</a></h2>
<p>microTVM aims to achieve these design goals:</p>
<ol class="arabic simple">
<li><p><strong>Portable Code</strong>. microTVM can translate any Relay model into C code that can compile with only
a C standard library.</p></li>
<li><p><strong>Minimal Overhead</strong>. microTVM generates target-specific, highly optimized code. As much overhead
from the runtime should be removed.</p></li>
<li><p><strong>Accessible Code</strong>. microTVM considers C source code as a first-class output mechanism so that
it is easier for a firmware engineer to understand and tweak.</p></li>
</ol>
</div>
<div class="section" id="overview">
<h2><a class="toc-backref" href="#id5">Overview</a><a class="headerlink" href="#overview" title="永久链接至标题">¶</a></h2>
<p>microTVM requires changes at all levels of the TVM compiler stack. The following sub-sections enumerate
these changes at a high level, and follow-on sections discuss the specifics in more detail.</p>
<div class="section" id="modeling-target-platforms">
<h3><a class="toc-backref" href="#id6">Modeling Target Platforms</a><a class="headerlink" href="#modeling-target-platforms" title="永久链接至标题">¶</a></h3>
<p>TVM’s search-based optimization approach allows it to largely avoid system-level modeling of targets
in favor of experimental results. However, some modeling is necessary in order to ensure TVM is
comparing apples-to-apples search results, and to avoid wasting time during the search by attempting
to compile invalid code for a target.</p>
<p>microTVM models these parts of the target:</p>
<ul class="simple">
<li><p>The CPU used, through the <code class="docutils literal notranslate"><span class="pre">-mcpu</span></code> and <code class="docutils literal notranslate"><span class="pre">-march</span></code> target flags.</p></li>
<li><p>The presence or absence of accelerators, through the device components of the target (Currently
only the absence of accelerators can be expressed, but this mechanism should extend well).</p></li>
</ul>
<p>microTVM aims to model these parts of the target in the future:</p>
<ul class="simple">
<li><p>Memory, modeled as a set of disjoint memory spaces, each with a label and size and prefetch/flush
behavior. Some memory may be shared with accelerators.</p></li>
<li><p>Target runtime configuration (i.e. clock tree configuration, clock speed, etc). This is intended
only to contribute to the AutoTVM schedule key and not for any other use.</p></li>
</ul>
<p>At this time, TVM does not intend to model:</p>
<ul class="simple">
<li><p>Size, type, or relationship of caches, with the exception of prefetching or cache flushing.</p></li>
</ul>
</div>
<div class="section" id="tvm-targets-for-microtvm">
<h3><a class="toc-backref" href="#id7">TVM Targets for microTVM</a><a class="headerlink" href="#tvm-targets-for-microtvm" title="永久链接至标题">¶</a></h3>
<p>A central data structure in the compilation process is the <code class="docutils literal notranslate"><span class="pre">tvm::target::Target</span></code> class. TVM uses
Target to decide which TIR schedules to enable and how to configure the code generator. The Target
class should also uniquely identify the generated code for a particular operator, as autotuning
logs use it to rank measured performance (but see Future Work).</p>
<p>Targets are currently represented as strings structured similarly to command-line arguments. An
example target is shown below:</p>
<blockquote>
<div><p><code class="docutils literal notranslate"><span class="pre">c</span> <span class="pre">-keys=arm_cpu</span> <span class="pre">-mcpu=cortex-m7</span> <span class="pre">-model=stm32f746xx</span></code></p>
</div></blockquote>
<p>The relevant parts to microTVM are:</p>
<blockquote>
<div><ul class="simple">
<li><p>Code generator (<code class="docutils literal notranslate"><span class="pre">llvm</span></code> or <code class="docutils literal notranslate"><span class="pre">c</span></code>)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">-mcpu=cortex-m7</span></code>: used by TOPI to enable Cortex-M schedules, and, when the C source code
generator is selected, included in the output as a comment to help identify the code and
configure the downstream C compiler.</p></li>
</ul>
</div></blockquote>
</div>
<div class="section" id="runtime-and-executor-configuration-for-microtvm">
<h3><a class="toc-backref" href="#id8">Runtime and Executor configuration for microTVM</a><a class="headerlink" href="#runtime-and-executor-configuration-for-microtvm" title="永久链接至标题">¶</a></h3>
<p>When using microTVM, it’s important to use the C Runtime (<code class="docutils literal notranslate"><span class="pre">Runtime('crt')</span></code>), which is the runtime that works best on micro devices rather than the more dynamic C++ Runtime. Alongside this, there are two executors which you could use in combination with the C runtime:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">Executor(&quot;aot&quot;)</span></code> - The Ahead of Time (AOT) executor precompiles the network into a runnable function which you can add directly into your micro application</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Executor(&quot;graph&quot;,</span> <span class="pre">{&quot;link-params&quot;:</span> <span class="pre">True})</span></code> - The Graph executor provides a JSON representation of your network and requires the C Runtime’s system library to be generated to find functions in the function registry (<code class="docutils literal notranslate"><span class="pre">Runtime(&quot;crt&quot;,</span> <span class="pre">{&quot;system-lib&quot;:</span> <span class="pre">True})</span></code>). <code class="docutils literal notranslate"><span class="pre">{&quot;link-params&quot;:True}</span></code> enables parameters to be linked into the generated files rather than provided externally.</p></li>
</ul>
<p>These are specified when building a runtime module: <code class="docutils literal notranslate"><span class="pre">relay.build(...,</span> <span class="pre">runtime=...,</span> <span class="pre">executor=...)</span></code>.</p>
</div>
<div class="section" id="writing-schedules-for-microtvm">
<h3><a class="toc-backref" href="#id9">Writing Schedules for microTVM</a><a class="headerlink" href="#writing-schedules-for-microtvm" title="永久链接至标题">¶</a></h3>
<p>For operations scheduled on the CPU, microTVM initially plans to make use of specialized
instructions and extern (i.e. hand-optimized) functions to achieve good performance. In TVM, this
approach is generally accomplished through tensorization, in which TVM breaks a computation into
small pieces, and a TIR extern function accelerates each small piece.</p>
<p>TVM currently accommodates both approaches using <code class="docutils literal notranslate"><span class="pre">tir.call_extern</span></code>. First, a pragma is attached to
the schedule defining the extern function in portable C.</p>
<blockquote>
<div><p><code class="docutils literal notranslate"><span class="pre">sched[output].pragma(n,</span> <span class="pre">&quot;import_c&quot;,</span> <span class="pre">&quot;void</span> <span class="pre">call_asm(int32_t*</span> <span class="pre">a,</span> <span class="pre">int32_t*</span> <span class="pre">b)</span> <span class="pre">{</span> <span class="pre">/*</span> <span class="pre">...</span> <span class="pre">*/</span> <span class="pre">}&quot;)</span></code></p>
</div></blockquote>
<p>Next, <code class="docutils literal notranslate"><span class="pre">tensorize</span></code> is used to split the computation.</p>
<blockquote>
<div><p><code class="docutils literal notranslate"><span class="pre">sched[output].tensorize(owi,</span> <span class="pre">gemm)</span></code></p>
</div></blockquote>
<p>There are a couple of caveats to this approach, all which could be resolved by linking generated
code against external libraries:</p>
<ul class="simple">
<li><p>Inline assembly is compiler-specific. While Clang and GCC have standardized on one syntax, this
may not be portable to other compilers. SDKs solve this by conditionally including a header file
depending on the compiler being used. However, taking this approach means that the generated code
needs additional compiler flags (i.e. <code class="docutils literal notranslate"><span class="pre">-Isystempath/to/header</span></code>).</p></li>
<li><p>It may be helpful to reference helper functions from the generated code (e.g. to inline common
sequences of hand-optimized assembly).</p></li>
<li><p>Finally, the extern function invoked may be wholly written in an external library. If those
functions can be wholly inlined, this caveat is the same as the previous. If not, then additional
C code needs to be compiled and linked against the operator.</p></li>
</ul>
<p>At present, microTVM presumes that all eligible schedules can be compiled. This means that the user-
supplied project (see next section) must include all libraries that are used by the generated code.
When not using autotuning, TVM randomly chooses a fallback schedule, so all libraries would need to
be supported. When using autotuning, TVM selects the best-performing schedule, so only that library
is needed. There isn’t currently a way to force TVM to pick a particular schedule outside of
autotuning logs, but that would be a good addition.</p>
<p>Finally, when using the <code class="docutils literal notranslate"><span class="pre">llvm</span></code> backend, the process is similar except that LLVM bitcode is included
in the generated code (with an <code class="docutils literal notranslate"><span class="pre">import_llvm</span></code> pragma). LLVM bitcode provides a portable way to call
inline assembly. However, it may be more complex to call external C functions, and helper functions
are of course not easy to use from LLVM bitcode.</p>
</div>
<div class="section" id="executing-models">
<h3><a class="toc-backref" href="#id10">Executing Models</a><a class="headerlink" href="#executing-models" title="永久链接至标题">¶</a></h3>
<p>The TVM compiler traditionally outputs three pieces:</p>
<ol class="arabic simple">
<li><p>Model operator implementations, as discussed above;</p></li>
<li><p>A model execution graph, encoded as JSON; and</p></li>
<li><p>Simplified parameters.</p></li>
</ol>
<p>To correctly execute the model, a Graph Executor needs to reconstruct the graph in memory, load the
parameters, and then invoke the operator implementations in the correct order.</p>
<p>microTVM supports two ways to do this:</p>
<ol class="arabic simple">
<li><p><strong>Host-Driven</strong>. The Graph Executor can run on the host and carry out execution by issuing
commands to the device using an RPC link with a UART-like transport.</p></li>
<li><p><strong>Standalone</strong>. A C Graph Executor is available to be compiled on-device, but it is not
particularly memory efficient. This way enables standalone execution without any attached host.</p></li>
</ol>
<p>Host-Driven is designed for experimenting with models on-device and, like AutoTVM, uses the RPC server to
drive computation on-device. Standalone is intended for deployment.</p>
<div class="section" id="host-driven-execution">
<h4>Host-Driven Execution<a class="headerlink" href="#host-driven-execution" title="永久链接至标题">¶</a></h4>
<p>In Host-Driven execution, the firmware binary is the following:</p>
<ol class="arabic simple">
<li><p>Generated operator implementations from TVM.</p></li>
<li><p>The TVM C runtime.</p></li>
<li><p>SoC-specific initialization.</p></li>
<li><p>The TVM RPC server.</p></li>
<li><p>(optional) Simplified Parameters.</p></li>
</ol>
<p>This firmware image is flashed onto the device and a GraphExecutor instance is created on the host.
The GraphExecutor drives execution by sending RPC commands over a UART:</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="https://raw.githubusercontent.com/tvmai/web-data/main/images/dev/microtvm_host_driven.svg"><img alt="https://raw.githubusercontent.com/tvmai/web-data/main/images/dev/microtvm_host_driven.svg" src="https://raw.githubusercontent.com/tvmai/web-data/main/images/dev/microtvm_host_driven.svg" width="85%" /></a>
</div>
</div>
<div class="section" id="standalone-execution">
<h4>Standalone Execution<a class="headerlink" href="#standalone-execution" title="永久链接至标题">¶</a></h4>
<p>In Standalone execution, the GraphExecutor is instantiated on device:</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="https://raw.githubusercontent.com/tvmai/web-data/main/images/dev/microtvm_standalone.svg"><img alt="https://raw.githubusercontent.com/tvmai/web-data/main/images/dev/microtvm_standalone.svg" src="https://raw.githubusercontent.com/tvmai/web-data/main/images/dev/microtvm_standalone.svg" width="85%" /></a>
</div>
</div>
</div>
<div class="section" id="microtvm-firmware">
<h3><a class="toc-backref" href="#id11">microTVM Firmware</a><a class="headerlink" href="#microtvm-firmware" title="永久链接至标题">¶</a></h3>
<p>We can now discuss how microTVM firmware should behave. An important task common to both model
execution strategies is configuring the SoC to match the way it performs in production. microTVM
considers this task project- and SoC-dependent. Whether for AutoTVM, host-driven model inference, or
in standalone deployment, the user is expected to supply a project whose main() does the following:</p>
<ol class="arabic simple">
<li><p>Configure the SoC to match deployment performance.</p></li>
<li><p>Initialize the TVM C Runtime.</p></li>
</ol>
<p>When configuring for host-driven inference or AutoTVM, the remaining tasks are well-defined:</p>
<ol class="arabic simple" start="3">
<li><p>Initialize a transport (i.e. a UART) for use with the TVM RPC server.</p></li>
<li><p>Launch the TVM RPC Server.</p></li>
</ol>
<p>When configuring for standalone deployment, the firmware needs to:</p>
<ol class="arabic simple">
<li><p>Instantiate the system library by calling the <code class="docutils literal notranslate"><span class="pre">runtime.SystemLib</span></code> PackedFunc.</p></li>
<li><p>Instantiate a GraphExecutor passing the system library module.</p></li>
<li><p>Configure parameters and inputs as needed.</p></li>
<li><p>Run the model.</p></li>
</ol>
</div>
<div class="section" id="parts-of-a-microtvm-binary">
<h3><a class="toc-backref" href="#id12">Parts of a microTVM Binary</a><a class="headerlink" href="#parts-of-a-microtvm-binary" title="永久链接至标题">¶</a></h3>
<p>To summarize, a microTVM firwmare binary image must contain these parts:</p>
<ol class="arabic simple">
<li><p>Operator implementations, produced by TVM.</p></li>
<li><p>The TVM C runtime library, supplied by TVM as a static library.</p></li>
<li><p>SoC Initialization, supplied by the user.</p></li>
</ol>
<p>For Host-driven model execution, firmware also needs:</p>
<ol class="arabic simple" start="4">
<li><p>The TVM RPC Server library.</p></li>
</ol>
<p>For Standalone model execution, firmware also needs:</p>
<ol class="arabic simple" start="4">
<li><p>The TVM C GraphExecutor library, supplied by TVM as a static library.</p></li>
<li><p>The remaining compiler outputs (Simplified Parameters and Graph JSON).</p></li>
</ol>
</div>
<div class="section" id="the-automated-build-flow">
<h3><a class="toc-backref" href="#id13">The Automated Build Flow</a><a class="headerlink" href="#the-automated-build-flow" title="永久链接至标题">¶</a></h3>
<p>Once code generation is complete, <code class="docutils literal notranslate"><span class="pre">tvm.relay.build</span></code> returns a <code class="docutils literal notranslate"><span class="pre">tvm.runtime.Module</span></code> and the
user can save the generated C source or binary object to a <code class="docutils literal notranslate"><span class="pre">.c</span></code> or <code class="docutils literal notranslate"><span class="pre">.o</span></code> file. From this point, TVM
can theoretically step back and the user can compile and run the code separately.</p>
<p>However, for AutoTVM, TVM needs some automated flow to handle the following tasks:</p>
<ol class="arabic simple">
<li><p>Integrate operator implementations, the TVM C Runtime library, and the TVM RPC Server library into the
firmware project containing user-supplied SoC Initialization.</p></li>
<li><p>Build the resulting project.</p></li>
<li><p>Program the built firmware onto a (specific) attached device.</p></li>
<li><p>Identify the serial port or other transport to be used by TVM to drive remote execution.</p></li>
</ol>
<p>At present, TVM expects the user to supply an implementation of the <code class="docutils literal notranslate"><span class="pre">tvm.micro.Compiler</span></code>,
<code class="docutils literal notranslate"><span class="pre">tvm.micro.Flasher</span></code>, and <code class="docutils literal notranslate"><span class="pre">tvm.micro.Transport</span></code> interfaces. TVM then:</p>
<ol class="arabic simple">
<li><p>Builds each piece separately as a library.</p></li>
<li><p>Builds the libraries into a binary firmware image.</p></li>
<li><p>Programs the firmware image onto an attached device.</p></li>
<li><p>Opens a serial port to serve as the RPC server transport.</p></li>
</ol>
<p>This design was chosen to reduce build times for microTVM (the common libraries need to be built
only once per candidate operator implemmentation). In practice, these projects are extremely small
and compile relatively quickly. Compared with the added complexity of this tighter build integration
with TVM, the performance gains are likely not worth it. A future design will consolidate the build
tasks into a single step and narrow the interface to provide a better integration.</p>
</div>
<div class="section" id="measuring-operator-performance">
<h3><a class="toc-backref" href="#id14">Measuring operator performance</a><a class="headerlink" href="#measuring-operator-performance" title="永久链接至标题">¶</a></h3>
<p>The TVM C runtime depends on user-supplied functions to measure time on-device. Users should implement
<code class="docutils literal notranslate"><span class="pre">TVMPlatformTimerStart</span></code> and <code class="docutils literal notranslate"><span class="pre">TVMPlatformTimerStop</span></code>. These functions should measure wall clock time, so there
are some pitfalls in implementing these functions:</p>
<ol class="arabic simple">
<li><p>If the CPU could halt or sleep during a computation (i.e. if it is being done on an accelerator),
a cycle counter should likely not be used as these tend to stop counting while the CPU is asleep.</p></li>
<li><p>The granularity of these functions can be relaxed as needed to extend the range of the timer
device. However, if granularity is too coarse, a sub-optimal schedule may be used.</p></li>
<li><p>An error should be raised if the timer overflows.</p></li>
<li><p>The timer should not interrupt computation unless absolutely necessary. Doing so may affect the
accuracy of the results.</p></li>
<li><p>Calibrating the output against a wall clock is ideal, but it will likely be too cumbersome. A
future PR could enable some characterization of the platform timer by, e.g., measuring the internal
oscillator against a reference such as an external crystal.</p></li>
</ol>
</div>
</div>
<div class="section" id="future-work">
<h2><a class="toc-backref" href="#id15">Future Work</a><a class="headerlink" href="#future-work" title="永久链接至标题">¶</a></h2>
<div class="section" id="ahead-of-time-runtime">
<h3><a class="toc-backref" href="#id16">Ahead-of-Time Runtime</a><a class="headerlink" href="#ahead-of-time-runtime" title="永久链接至标题">¶</a></h3>
<p>A limitation of the Graph Executor is the amount of memory overhead required in parsing the JSON.
The current implementation contributes significantly to the dynamic memory usage of microTVM,
limiting its utility. An ahead-of-time runtime can avoid the need for any Graph JSON parsing and
improve inference speed by generating C code to call the generated operator implementations directly
rather than relying on a data-driven approach with the Graph Executor.</p>
</div>
<div class="section" id="memory-planning">
<h3><a class="toc-backref" href="#id17">Memory Planning</a><a class="headerlink" href="#memory-planning" title="永久链接至标题">¶</a></h3>
<p>The current memory planner attempts to limit the number of <code class="docutils literal notranslate"><span class="pre">TVMBackendDeviceAlloc()</span></code> calls
issued for intermediate tensors only. Because scratchpads can vary widely, and because the planner
coalesces memory allocations within 16x of each other, this strategy typically results in high
peak memory usage.</p>
</div>
<div class="section" id="heterogeneous-execution">
<h3><a class="toc-backref" href="#id18">Heterogeneous Execution</a><a class="headerlink" href="#heterogeneous-execution" title="永久链接至标题">¶</a></h3>
<p>Newer Cortex-M SoCs can contain multiple CPUs and onboard ML accelerators.</p>
</div>
<div class="section" id="autotuning-target">
<h3><a class="toc-backref" href="#id19">Autotuning Target</a><a class="headerlink" href="#autotuning-target" title="永久链接至标题">¶</a></h3>
<p>As discussed previously,</p>
</div>
</div>
</div>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../index.html">tvm-cn</a></h1>








<h3>导航</h3>
<p class="caption"><span class="caption-text">Getting Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../install/index.html">Installing TVM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../contribute/index.html">Contributor Guide</a></li>
</ul>
<p class="caption"><span class="caption-text">User Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../how_to/index.html">How To Guides</a></li>
</ul>
<p class="caption"><span class="caption-text">Developer Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../dev/tutorial/index.html">Developer Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dev/how_to/how_to.html">Developer How-To Guide</a></li>
</ul>
<p class="caption"><span class="caption-text">Architecture Guide</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="index.html">Design and Architecture</a></li>
</ul>
<p class="caption"><span class="caption-text">Topic Guides</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../topic/microtvm/index.html">microTVM: TVM on bare-metal</a></li>
<li class="toctree-l1"><a class="reference internal" href="../topic/vta/index.html">VTA: Versatile Tensor Accelerator</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../index.html">Documentation overview</a><ul>
  <li><a href="index.html">Design and Architecture</a><ul>
      <li>Previous: <a href="security.html" title="上一章">Security Guide</a></li>
      <li>Next: <a href="microtvm_project_api.html" title="下一章">microTVM Project API</a></li>
  </ul></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">快速搜索</h3>
    <div class="searchformwrapper">
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" />
      <input type="submit" value="转向" />
    </form>
    </div>
</div>
<script>$('#searchbox').show(0);</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2022, HyperAI.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 3.2.1</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
      |
      <a href="../_sources/arch/microtvm_design.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>